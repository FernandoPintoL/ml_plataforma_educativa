# ‚ùì Preguntas Frecuentes (FAQ)

## General

### ¬øRealmente necesito tener dos servicios separados?

**S√≠**, por estas razones:

| Aspecto | Monol√≠tico | Separado |
|---------|-----------|----------|
| Si ML falla | ‚ùå Derriba todo | ‚úÖ Laravel sigue activo |
| Escalabilidad | ‚ùå Escalar todo | ‚úÖ Escalar solo ML |
| Actualizaciones | ‚ùå Todo juntos | ‚úÖ Independiente |
| Recursos | ‚ùå M√°s caros | ‚úÖ Optimizados |
| Debugging | ‚ùå M√°s complejo | ‚úÖ Aislado |

En producci√≥n, si los modelos consumen mucha memoria, puedes tener un servicio de ML escalado sin afectar Laravel.

### ¬øCu√°ndo usar monol√≠tico?

Solo si:
- Proyecto muy peque√±o (< 50 estudiantes)
- Modelos muy simples
- Sin requisitos de escalabilidad
- Equipo de 1-2 personas

Mejor: Hacer separados desde el inicio.

---

## Deployment en Railway

### ¬øCu√°nto cuesta en Railway?

| Recurso | Costo |
|---------|-------|
| Primer mes | $5 cr√©dito gratis |
| Desarrollo | $0 (tier gratuito limitado) |
| Producci√≥n | ~$5-20/mes por servicio |
| Cada 500GB/mes | $0.50 |

**Nuestro caso t√≠pico:**
- 1 vCPU + 512MB RAM = $5/mes
- PostgreSQL = $15/mes (opcional)
- Total = ~$20/mes para ambos servicios

Mucho m√°s barato que tener servidor propio.

### ¬øRailway redeploy autom√°ticamente en git push?

**S√≠**, es autom√°tico:

```
Tu push a GitHub ‚Üí Railway detecta ‚Üí Build ‚Üí Deploy
```

No necesitas hacer nada manualmente. Tarda ~3-5 minutos.

### ¬øPuedo probar antes de subir a Railway?

**S√≠**, dos opciones:

**Opci√≥n 1: Local r√°pido (recomendado)**
```bash
python -m venv venv
pip install -r requirements.txt
python -m uvicorn app:app --reload --port 8001
# Ir a http://localhost:8001/docs
```

**Opci√≥n 2: Local con Docker**
```bash
docker-compose up -d
curl http://localhost:8001/health
docker-compose down
```

Opci√≥n 1 es m√°s r√°pida (1 minuto vs 5 minutos).

### ¬øQu√© pasa si mi app crashea en Railway?

Railway:
1. Detecta el crash
2. Intenta reiniciar autom√°ticamente
3. Si sigue fallando, muestra error
4. Puedes ver logs en Dashboard

**Rollback manual:**
```
Dashboard ‚Üí Deployments ‚Üí Click en previous ‚Üí Redeploy
```

---

## Integraci√≥n con Laravel

### ¬øC√≥mo conectar Laravel a la API de ML?

```php
// config/services.php
'ml_api' => [
    'url' => env('ML_API_URL', 'http://localhost:8001'),
],

// En tu controller
use Illuminate\Support\Facades\Http;

$response = Http::post(
    config('services.ml_api.url') . '/supervisado/performance/predict',
    [
        'student_id' => $student->id,
        'features' => [3.5, 85, 10, 2.1, 45, 0.8, 1.2, 0.9, 0.85, 2.0]
    ]
);

if ($response->successful()) {
    $data = $response->json();
    // $data['risk_level'], $data['risk_score'], etc
}
```

En `.env`:
```
ML_API_URL=https://tu-ml-api.railway.app
```

### ¬øNecesito autenticaci√≥n entre servicios?

**Para desarrollo:** No necesitas

**Para producci√≥n:** Recomendado
- Token JWT simple
- API Key
- mTLS (complicado, no lo hagas)

Implementar:
```python
# En app.py
from fastapi.security import HTTPBearer
security = HTTPBearer()

@app.post("/supervisado/performance/predict")
async def predict_performance(
    request: PredictionRequest,
    credentials: HTTPAuthCredentials = Depends(security)
):
    # Validar token
    # ...
```

En Laravel:
```php
$response = Http::withToken($token)->post(...)
```

---

## Modelos ML

### ¬øD√≥nde van los modelos entrenados?

En la carpeta `trained_models/`:
```
trained_models/
‚îú‚îÄ‚îÄ performance_predictor.pkl    (Modelo Random Forest)
‚îú‚îÄ‚îÄ xgboost_model.pkl            (Modelo XGBoost)
‚îú‚îÄ‚îÄ kmeans_model.pkl             (Modelo K-Means)
‚îú‚îÄ‚îÄ lstm_model.h5                (Modelo LSTM)
‚îî‚îÄ‚îÄ ...
```

Los modelos deben estar entrenados **antes** de desplegar.

### ¬øPuedo actualizar modelos sin redeploy?

**Opci√≥n 1: Con redeploy (recomendado)**
```bash
# Reentrenar localmente
python supervisado/training/train_performance.py

# Guardar en trained_models/
# Push a GitHub
git add trained_models/
git commit -m "Updated models"
git push

# Railway redeploy autom√°tico
```

**Opci√≥n 2: Sin redeploy (avanzado)**
- Guardar modelos en base de datos
- API carga desde BD
- Requiere cambios en `app.py`

Opci√≥n 1 es m√°s simple.

### ¬øQu√© pasa si el modelo es muy grande?

Si el modelo > 200MB:

1. **Comprimirlo:**
   ```bash
   # Guardar sin compresi√≥n
   joblib.dump(model, 'model.pkl', compress=0)

   # Guardar con compresi√≥n
   joblib.dump(model, 'model.pkl', compress=3)
   ```

2. **Usar S3 en lugar de git:**
   - Guardar en AWS S3
   - App descarga en startup
   - M√°s complejo

3. **Segmentar el modelo:**
   - Splits en m√∫ltiples archivos
   - Cargar en paralelo

Para la mayor√≠a de casos, compresi√≥n level 3 es suficiente.

---

## Problemas Comunes

### Error: "Modelo no est√° entrenado"

**Causa:** El archivo `.pkl` o `.h5` no existe o est√° corrupto

**Soluci√≥n:**
```bash
# Reentrenar modelo
cd supervisado/training
python train_performance.py

# Verificar archivo
ls -lh ../models/trained_models/
```

### Error: "Database connection failed"

**Causa:** DATABASE_URL inv√°lida o BD no accesible

**Soluci√≥n en Railway:**
1. Ir a Dashboard
2. Verificar "Variables" tiene DATABASE_URL
3. Probar conexi√≥n:
   ```bash
   psql $DATABASE_URL -c "SELECT 1"
   ```

### Error: "Port 8001 already in use"

**En desarrollo local:**
```bash
# Usar otro puerto
python -m uvicorn app:app --port 8002

# O matar proceso
lsof -i :8001
kill -9 <PID>
```

**En Railway:** Autom√°tico, no hay problema

### Error: "Out of memory"

**Causas:**
- Modelos muy grandes (TensorFlow/PyTorch)
- Batch size muy grande
- Memory leak en c√≥digo

**Soluciones:**
1. Aumentar RAM del servicio en Railway
2. Reducir batch size
3. Usar cuantizaci√≥n en modelos
4. Usar `requirements-prod.txt` sin heavy deps

---

## Performance y Optimizaci√≥n

### ¬øQu√© tan r√°pido es la API?

Tiempos t√≠picos (desde cliente):

```
Health check:           ~50ms
Predicci√≥n simple:      100-500ms (depende modelo)
Batch 10 predicciones:  200-1000ms
Clustering:             500-2000ms (depende datos)
```

Railway ‚âà 50-100ms de latencia adicional por ubicaci√≥n.

### ¬øC√≥mo cachear predicciones?

```python
# En app.py con Redis
from redis import Redis

redis_client = Redis.from_url(REDIS_URL)

@app.post("/supervisado/performance/predict")
async def predict_performance(request: PredictionRequest):
    # Generar cache key
    cache_key = f"pred:{request.student_id}"

    # Intentar obtener del cache
    cached = redis_client.get(cache_key)
    if cached:
        return json.loads(cached)

    # Sino, calcular
    prediction = model.predict(...)

    # Guardar en cache por 1 hora
    redis_client.setex(cache_key, 3600, json.dumps(prediction))

    return prediction
```

### ¬øC√≥mo monitorear performance?

Railway proporciona:
- üìä CPU/RAM/Network en Dashboard
- üìù Logs en tiempo real
- üîî Alertas configurables

Para m√°s detalle:
```python
# Usar time tracking
import time

@app.post("/supervisado/performance/predict")
async def predict_performance(request: PredictionRequest):
    start = time.time()
    result = model.predict(...)
    duration = time.time() - start

    logger.info(f"Prediction took {duration:.3f}s")

    return result
```

---

## Seguridad

### ¬øDebo exponer todos los endpoints?

**En desarrollo:** S√≠, es √∫til para testing

**En producci√≥n:** Considera:
- `/docs` - Documenta toda tu API (√∫til, pero puede leakear info)
- `/health` - Debe estar p√∫blico (monitoreo)
- Endpoints de modelos - Proteger con auth si es sensible

Para deshabilitar docs:
```python
app = FastAPI(docs_url=None, redoc_url=None)
```

### ¬øC√≥mo proteger con JWT?

```python
from fastapi.security import HTTPBearer, HTTPAuthCredentials
from jose import JWTError, jwt

security = HTTPBearer()

async def verify_token(credentials: HTTPAuthCredentials = Depends(security)):
    try:
        payload = jwt.decode(
            credentials.credentials,
            SECRET_KEY,
            algorithms=[ALGORITHM]
        )
        return payload
    except JWTError:
        raise HTTPException(status_code=401, detail="Invalid token")

@app.post("/supervisado/performance/predict")
async def predict_performance(
    request: PredictionRequest,
    token_data = Depends(verify_token)
):
    # Token validado, continuar
    ...
```

---

## Maintenance

### ¬øC√≥mo actualizar dependencias?

```bash
# Ver actualizaciones disponibles
pip list --outdated

# Actualizar espec√≠fico
pip install --upgrade scikit-learn

# Actualizar todo (cuidado!)
pip install --upgrade -r requirements.txt

# Verificar cambios
python -m uvicorn app:app --reload

# Commit y push
git add requirements.txt
git commit -m "Update dependencies"
git push
```

### ¬øC√≥mo hacer backup de modelos?

```bash
# Opci√≥n 1: GitHub (si < 100MB)
git add trained_models/
git commit -m "Backup models"
git push

# Opci√≥n 2: AWS S3
aws s3 cp trained_models/ s3://mi-bucket/ --recursive

# Opci√≥n 3: Manual desde Railway
# Dashboard ‚Üí archivos ‚Üí descargar
```

### ¬øCada cu√°nto actualizar modelos?

Depende:
- **Semanal**: Si hay datos nuevos y cambios importantes
- **Mensual**: Reentrenamiento est√°ndar
- **Trimestral**: Performance review

Recomendaci√≥n: **Semanal o mensual** para educaci√≥n (datos cambian frecuentemente).

---

## Testing

### ¬øC√≥mo probar endpoints?

**Opci√≥n 1: test_api.py (r√°pido)**
```bash
python test_api.py
```

**Opci√≥n 2: curl**
```bash
curl -X POST "http://localhost:8001/supervisado/performance/predict" \
  -H "Content-Type: application/json" \
  -d '{"student_id":1,"features":[3.5,85,10,2.1,45,0.8,1.2,0.9,0.85,2.0]}'
```

**Opci√≥n 3: /docs (interfaz)**
```
http://localhost:8001/docs
```

**Opci√≥n 4: Postman**
- Importar endpoint
- Guardar en Postman Cloud
- Compartir con equipo

---

## Soporte

### ¬øD√≥nde reporto bugs?

1. **En desarrollo:**
   - Ver logs: `docker-compose logs ml-api`
   - Ejecutar `test_api.py`
   - Revisar `DEPLOYMENT.md` troubleshooting

2. **En Railway:**
   - Ir a Dashboard ‚Üí Logs
   - Ver hist√≥rico de deployments
   - Revisar variables de entorno

3. **Comunidad:**
   - Railway Docs: https://docs.railway.app
   - FastAPI Docs: https://fastapi.tiangolo.com
   - Stack Overflow

---

## Referencia R√°pida

| Comando | Uso |
|---------|-----|
| `python -m uvicorn app:app --reload` | Desarrollo local |
| `docker-compose up -d` | Testing con Docker |
| `python test_api.py` | Probar endpoints |
| `curl http://localhost:8001/health` | Health check |
| `http://localhost:8001/docs` | API docs interactivos |
| `git push` | Deploy autom√°tico en Railway |

---

**¬øTu pregunta no est√° aqu√≠?**

Revisar:
1. `QUICK_START.md` - Gu√≠a r√°pida
2. `DEPLOYMENT.md` - Gu√≠a completa
3. `SETUP_SUMMARY.md` - Referencia t√©cnica
4. `/docs` en tu API - Documentaci√≥n autom√°tica
